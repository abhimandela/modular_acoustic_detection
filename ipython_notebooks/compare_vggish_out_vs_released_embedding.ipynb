{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to forums, the VGGish model code was not the original code used to generate the released embeddings.  This code serves to compare the output of the VGGish model against the embedding.\n",
    "\n",
    "See https://groups.google.com/forum/#!topic/audioset-users/EITl3rcNDI8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "from scipy.spatial.distance import euclidean, cosine\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import six\n",
    "import tensorflow as tf\n",
    "\n",
    "from youtube_audioset import get_data, get_recursive_sound_names, get_all_sound_names\n",
    "from youtube_audioset import explosion_sounds, motor_sounds, wood_sounds, human_sounds, nature_sounds\n",
    "\n",
    "from youtube_audioset import download_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('./externals/tensorflow_models/research/audioset/'))\n",
    "\n",
    "from vggish_input import wavfile_to_examples, waveform_to_examples\n",
    "import vggish_input\n",
    "import vggish_params\n",
    "import vggish_postprocess\n",
    "import vggish_slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is an all silent clip\n",
    "\n",
    "ytid = 'RhSLUvQ_LuM'\n",
    "yt_start = 30\n",
    "yt_end = 40\n",
    "\n",
    "audio_file_path = 'sounds/audioset/'+ ytid+'-'+str(yt_start)+'-'+str(yt_end)+'.wav'\n",
    "\n",
    "download_clip(ytid, yt_start, yt_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "examples_batch = wavfile_to_examples(audio_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sr, wav_data = wavfile.read(audio_file_path)\n",
    "\n",
    "print \"Energy of signal:\", np.square(wav_data).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is confirmed that the audio signal only contains zero samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copied from https://github.com/tensorflow/models/blob/master/research/audioset/vggish_inference_demo.py\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    'wav_file', None,\n",
    "    'Path to a wav file. Should contain signed 16-bit PCM samples. '\n",
    "    'If none is provided, a synthetic sound is used.')\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    'checkpoint', './externals/tensorflow_models/research/audioset/vggish_model.ckpt',\n",
    "    'Path to the VGGish checkpoint file.')\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    'pca_params', './externals/tensorflow_models/research/audioset/vggish_pca_params.npz',\n",
    "    'Path to the VGGish PCA parameters file.')\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    'tfrecord_file', None,\n",
    "    'Path to a TFRecord file where embeddings will be written.')\n",
    "\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Copied from https://github.com/tensorflow/models/blob/master/research/audioset/vggish_inference_demo.py\n",
    "\n",
    "# Prepare a postprocessor to munge the model embeddings.\n",
    "pproc = vggish_postprocess.Postprocessor(FLAGS.pca_params)\n",
    "\n",
    "# If needed, prepare a record writer to store the postprocessed embeddings.\n",
    "writer = tf.python_io.TFRecordWriter(\n",
    "  FLAGS.tfrecord_file) if FLAGS.tfrecord_file else None\n",
    "\n",
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "    # Define the model in inference mode, load the checkpoint, and\n",
    "    # locate input and output tensors.\n",
    "    vggish_slim.define_vggish_slim(training=False)\n",
    "    vggish_slim.load_vggish_slim_checkpoint(sess, FLAGS.checkpoint)\n",
    "    features_tensor = sess.graph.get_tensor_by_name(\n",
    "        vggish_params.INPUT_TENSOR_NAME)\n",
    "    embedding_tensor = sess.graph.get_tensor_by_name(\n",
    "        vggish_params.OUTPUT_TENSOR_NAME)\n",
    "\n",
    "    # Run inference and postprocessing.\n",
    "    [embedding_batch] = sess.run([embedding_tensor],\n",
    "                                 feed_dict={features_tensor: examples_batch})\n",
    "    print(embedding_batch)\n",
    "    postprocessed_batch = pproc.postprocess(embedding_batch)\n",
    "    print(postprocessed_batch)\n",
    "\n",
    "    # Write the postprocessed embeddings as a SequenceExample, in a similar\n",
    "    # format as the features released in AudioSet. Each row of the batch of\n",
    "    # embeddings corresponds to roughly a second of audio (96 10ms frames), and\n",
    "    # the rows are written as a sequence of bytes-valued features, where each\n",
    "    # feature value contains the 128 bytes of the whitened quantized embedding.\n",
    "    seq_example = tf.train.SequenceExample(\n",
    "        feature_lists=tf.train.FeatureLists(\n",
    "            feature_list={\n",
    "                vggish_params.AUDIO_EMBEDDING_FEATURE_NAME:\n",
    "                    tf.train.FeatureList(\n",
    "                        feature=[\n",
    "                            tf.train.Feature(\n",
    "                                bytes_list=tf.train.BytesList(\n",
    "                                    value=[embedding.tobytes()]))\n",
    "                            for embedding in postprocessed_batch\n",
    "                        ]\n",
    "                    )\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    print(seq_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results should be identical for each embedding of every 1 second interval.  So let's just look at the first result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "processed_embedding = postprocessed_batch[0,:]\n",
    "\n",
    "processed_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "silence_embedding = joblib.load('parameter/silence_embedding.pkl')\n",
    "\n",
    "silence_embedding.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cosine(silence_embedding, processed_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "euclidean(silence_embedding, processed_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine distance is low but euclidean distance is very high."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
